project_code = 'OT'

[grids]
	units = 'meter'
	method = 'ODS'
	origin = [650200, 4766170, -500]
	dimensions = [100, 101, 68]
	spacing = 25

	[grids.velocities]
		source = 'local'
		homogeneous = false
		vp = 'velocities/vp'
		vs = 'velocities/vs'
		format = 'NLLOC'

    [grids.travel_time_h5]
        fname = 'travel_time_tables.h5f'


# Not used, definition similar to velocity grids
# 	[grids.density]
#		homogeneous = false

[time_zone]
	type = 'UTC_offset'                # either 'UTC_offset or 'time_zone'
	time_zone_code = 'ULN'             # time zone code. If type=time_zone, time_zone_code must be a valid pytz timezone
	offset = 8                         # time offset in hours, ignored if type=time_zone

[sensors]
	source     = 'local'
	units      = 'meter'
	format     = 'csv'
	path       = 'sensors.csv'
    black_list = ['23', '31', '32', '100', '102', '107', '88', '90', '77']

[data_local]
    ms_chunks = '/home/phil/data/oyu/spp_common/synthetic/chunks/*.mseed'
    ms_10s    = '/home/phil/data/oyu/spp_common/synthetic/simdat10s.mseed'
    xml_10s   = '/home/phil/data/oyu/spp_common/synthetic/simdat10s.xml'

[processing_flow]
    # this describes the kafka channel to which the message should be routed
    [processing_flow.automatic]
        steps=[['location'], ['interloc'], ['picker'], ['location'], 
               ['magnitude'], ['event_database']]
    [processing_flow.interactive]
        steps=[['location'], ['magnitude'], ['event_database', 'ui_notification']]


[data_connector]
    source  = 'remote'
    path    = 'http://10.95.74.35:8002/ims-database-server/databases/mgl'
    # source = 'local'
    # path = '/Users/jpmercier/.spp/data/'
    kafka_consumer_topic = 'data_connector'
    kafka_feedback_topic = 'data_connector'
    # kafka_producer_topic = ['continuous_db_handler', 'mseed_1sec']
    # kafka_producer_topic = 'mseed_1sec'
    kafka_producer_topic =  'picker'
    log_topic = 'data_connector'
    log_file_name = 'data_connector.log'

[initializer]
    kafka_consumer_topic = 'initializer'
    # kafka_producer_topic = 'interloc'
    log_topic = 'initializer'
    log_file_name = 'initializer.log'
    [initializer.filter]
        freqmin=100
        freqmax=1000
    [initializer.window_size]
        start=-0.3    # seconds before event time
        end=0.9       # seconds after event time

[mseed_decomposer]  
    kafka_producer_topic = 'mseed_blocks'

[transformer]
    kafka_consumer_topic = 'mseed_blocks'
    kafka_producer_topic = 'mseed_1sec'

[interloc]
    threads = 4
    debug = 0
    wlen_seconds = 1.0
    dsr = 3000.0
    threshold = 0.2
    kafka_consumer_topic = 'interloc'
    # kafka_producer_topic = 'picker'
    log_topic = 'interloc'
    log_file_name = 'interloc.log'

[create_event]
    log_topic = 'create_event'
    log_file_name = 'create_event.log'

[picker]
    snr_threshold = 6
    residual_tolerance = 10e-3
    p_s_tolerance = 5e-3  # P and S must be at least separated by p_s_tolerance
    kafka_consumer_topic = 'picker'
    # kafka_producer_topic = 'location'
    log_topic = 'automatic picker'
    log_file_name = 'automatic_picker.log'

    [picker.waveform_filter]
        frequency_min = 100
        frequency_max = 1000

    [picker.p_wave]
        [picker.p_wave.search_window]
            start = -70e-3        # start of search window relative to predicted arrival time (s)
            end = 20e-3           # end of the search window relative to predicted arrival time (s)
            resolution = 0.001   # time resolution of search window (s)

        [picker.p_wave.snr_window]
            noise = 0.005         # length of the window for noise energy measurement
            signal = 0.01        # length of the window for signal energy measurement

        [picker.s_wave.search_window]
            start = -70e-3        # start of search window relative to predicted arrival time (s)
            end = 20e-3           # end of the search window relative to predicted arrival time (s)
            resolution = 0.001   # time resolution of search window (s)

        [picker.s_wave.snr_window]
            noise = 0.005         # length of the window for noise energy measurement
            signal = 0.01        # length of the window for signal energy measurement

[nlloc]
	nll_base = 'NLL'
	locsig = 'Oyu Tolgoi Geotechnical Monitoring Team'
	loccom = 'Prototype'
    residual_tolerance = 10e-3
	# locsearch = 'OCT 20 20 30 1e-6 50000 1000 0 1'
	# locsearch = 'OCT 10 10 10 1e-6 20000 1000 0 1'
	locsearch = 'OCT 5 5 5 1e-6 5000 500 0 1'
	locmeth = 'EDT_OT_WT 9999.0 4 -1 -1 -1 0'
    locgau = ''
	kafka_consumer_topic = 'location'
	# kafka_producer_topic = 'magnitude'
	log_file_name = 'hypocenter_location_nlloc.log'
	log_topic = 'hypocenter Location NonLinLoc'

[magnitude]
	ttpath = 'None'
	only_triaxial = true
	density = 2700
    min_dist = 20
    win_length = 0.02
    # len_spectrum is specified in exponent of 2. For instance 14 is equal to 2 ** 14
    len_spectrum_exponent = 14
    # could instead specify <len_spectrum> directly
    freq = 100
    kafka_consumer_topic = 'magnitude'
    # kafka_producer_topic = 'event_database'
    log_file_name = 'magnitude_calculation.log'
    log_topic = 'moment magnitude calculation'

[focal_mechanism]
    npolmin = 8
    max_agap = 180
    #max_agap = 90
    max_pgap = 60
    dang = 5
    nmc = 30
    maxout = 500
    badfrac = 0.1
    delmax = 120
    cangle = 45
    prob_max = 0.25
    #kafka_consumer_topic = 'magnitude'
    #kafka_producer_topic = 'event_database'
    log_file_name = 'focal_mechanism_calculation.log'
    log_topic = 'focal mechanism calculation'

[kafka]
    brokers = 'broker:9092'
    group_id = 'seismic_processing_platform'
    threads = 3 

[airflow]
    description = 'Data Loader'
    catchup = false

[logging]
    log_directory = '/var/log/spp/'
    log_level = 'INFO'
    log_format = '[%(levelname)s] %(asctime)s <%(name)s> <%(thread)d> %(message)s'
    log_filename = 'spp.log'
    logger_name = 'spp'
    log_destination = 'stdout'

[event_database_handler]
	kafka_consumer_topic = 'event_database'
    log_file_name = 'event_database_handler.log'
    log_topic = 'event database handler'

[event_db]
    uri = 'mongodb://localhost:27017/'
    name = 'OT'
    trace_collection = 'traces'
    event_collection = 'events'
    events_inuse_collection = 'event_in_use'
    in_use_ttl = 1
    filestore_base_dir = '/var/ot/'
    kafka_topic = 'event_database'
    log_file_name = 'event_db_insertion_update.log'
    log_topic = 'insertion and update into event DB'

[continuous_db]
  	uri = 'mongodb://mongo-node-001:27017/'
  	db_name = 'seismic'
  	traces_collection = 'traces_json'
  	events_collection = 'events'
  	events_inuse_collection = 'events_inuse'
  	filestore_base_dir = '/Users/hanee/Rio_Tinto/data/'
  	inuse_ttl = 300

[seismic_api]
	base_url = 'http://sppkube.eastus.cloudapp.azure.com/api/v1/'

[redis_db]
	host = 'localhost'
	password = ''
	port = 6379
	db = 0

[redis_extra]
	ttl=600

